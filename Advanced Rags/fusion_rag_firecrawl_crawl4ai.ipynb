{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHNLAMYpLPA1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "nW7Ru5meLL5V"
      },
      "outputs": [],
      "source": [
        "!pip install -q fastembed firecrawl-py langchain_community langchain-mistralai crawl4ai[all] playwright crawl4ai langchain-groq langchain-core lxml langchain langchain-qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MYee6ykT-_a",
        "outputId": "9a2a3426-4684-4aec-f22f-a7e631dc2f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to install browsers\n",
            "Error: \n",
            "╔═════════════════════════════════════════════════════════════════╗\n",
            "║ ATTENTION: \"chrome\" is already installed on the system!         ║\n",
            "║                                                                 ║\n",
            "║ \"chrome\" installation is not hermetic; installing newer version ║\n",
            "║ requires *removal* of a current installation first.             ║\n",
            "║                                                                 ║\n",
            "║ To *uninstall* current version and re-install latest \"chrome\":  ║\n",
            "║                                                                 ║\n",
            "║ - Close all running instances of \"chrome\", if any               ║\n",
            "║ - Use \"--force\" to install browser:                             ║\n",
            "║                                                                 ║\n",
            "║     playwright install --force chrome                           ║\n",
            "║                                                                 ║\n",
            "║ <3 Playwright Team                                              ║\n",
            "╚═════════════════════════════════════════════════════════════════╝\n"
          ]
        }
      ],
      "source": [
        "!playwright install chrome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "A9edJkDg4vM2"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.firecrawl import FireCrawlLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "wQZ9qtzKLRJl"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "loader = FireCrawlLoader(\n",
        "    api_key=userdata.get('FIRECRAWL'),url='https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba', mode=\"scrape\"\n",
        ")\n",
        "doc = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "BO__o6BhMArz"
      },
      "outputs": [],
      "source": [
        "from crawl4ai import AsyncWebCrawler\n",
        "\n",
        "url1='https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "t08yvUenTYqB"
      },
      "outputs": [],
      "source": [
        "url = 'https://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html'\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "\n",
        "async def parse_web(url):\n",
        "    # Create an instance of AsyncWebCrawler\n",
        "    async with AsyncWebCrawler(verbose=False) as crawler:\n",
        "        # Run the crawler on a URL\n",
        "        result = await crawler.arun(url=url)\n",
        "\n",
        "        # Print the extracted content\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmYihUmr2mfx",
        "outputId": "13271178-5317-4365-84b5-675ce8c2ea64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.4.23\n",
            "[FETCH]... ↓ https://thegrigorian.medium.com/understanding-vgg-... | Status: True | Time: 0.02s\n",
            "[SCRAPE].. ◆ Processed https://thegrigorian.medium.com/understanding-vgg-... | Time: 965ms\n",
            "[COMPLETE] ● https://thegrigorian.medium.com/understanding-vgg-... | Status: True | Total: 1.21s\n",
            "[INIT].... → Crawl4AI 0.4.23\n",
            "[FETCH]... ↓ https://books.toscrape.com/catalogue/scott-pilgrim... | Status: True | Time: 0.05s\n",
            "[SCRAPE].. ◆ Processed https://books.toscrape.com/catalogue/scott-pilgrim... | Time: 201ms\n",
            "[COMPLETE] ● https://books.toscrape.com/catalogue/scott-pilgrim... | Status: True | Total: 0.36s\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema import Document\n",
        "docs = []\n",
        "\n",
        "for i in [url1,url]:\n",
        "  x = await parse_web(i)\n",
        "  docs.append(Document(page_content=str(x.markdown_v2),metadata=x.metadata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "QLUh3F5l3PUJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import Runnable\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "#from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "from typing import cast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "dAlfTvRe5Qmw"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "os.environ[\"MISTRAL_API_KEY\"] =  userdata.get('MISTRAL_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "nET34Rn05XJ4"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "ZAmp-8z0U6nA"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "\n",
        "embeddings = FastEmbedEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGWVnYev-UqH",
        "outputId": "2a00eaf8-e24d-4954-feb0-5992eccfd0e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['98e1a35f0b9548e692445119e41fdfa0',\n",
              " '3d58433d9055436f98073c14c057becc',\n",
              " '968e91b8287b4bdc8301b95da85fe9cd',\n",
              " 'fa1f10fa63ec4b8f8bff286ab0260813',\n",
              " '978660bd2fc3431b84d582aec1f41913',\n",
              " '613a4fd89e15486c8bb0dffa5692b50a',\n",
              " 'aa200e0a7b1543c5b0f084794e0f6bf8',\n",
              " 'a21d0652265f4034b4240317cfe43855',\n",
              " 'e4d3def2b30044c3a485069056cbeb56',\n",
              " '7f0f6665c1dc41a9b74dd262386ba8f2',\n",
              " '0659c2fc90674a1f99f6861c3fa14225',\n",
              " '6f7ec43085d64edaa5c6c6446b5c83ff',\n",
              " 'ae5cbd4a3af24f0f908663e764f4dbde',\n",
              " 'a6787c6238d94f3c87fd521af6d5ec0c',\n",
              " '88871048c35349edb85de19fee5f13d2',\n",
              " '6464711d91ec42c7847deeff82deb22a',\n",
              " '790632671d9a4cc9a56edb1725ad835c',\n",
              " '1e63130875714229bc137047bf0b21d4',\n",
              " 'efd8817cf3414cb38dd6aa54f0314e93',\n",
              " 'b25354c2b6494cdcab84bb1dc68a7ff3',\n",
              " '4aa57e41ffff4a25ad76aa9264f1efe4',\n",
              " '5ec15e362aed47c788413f2b62f3b27b',\n",
              " '9bebeafea2a146c88fd3ee07835899a9',\n",
              " '241b61f04c6e4c49a627428758bf8d9c',\n",
              " '325a1fa78902409684e98eae9af8b09e',\n",
              " '9381a097d0d94b538573e88f116507dc',\n",
              " 'a1c29ea1e6ca481b933e170039b1664c',\n",
              " 'b1d821aadf934dbbafa7b1e716dd4955',\n",
              " '92588d91c07a4d9c8c0e7e96cc18dc75',\n",
              " '0725e418e27640f8ab0754326b106cea',\n",
              " '68d8547c6ea440fa81f978c1c3bac06a',\n",
              " '4e546010a72345b698237be2df135807',\n",
              " 'd4dee4de93a845c0a2ef1a69014f3474',\n",
              " 'a89f88137ba94b6d84f37e3344cfd7e3',\n",
              " 'c15a7a290cfb4cd2b10ec4188f8d6cd3',\n",
              " '204e953de99542f6b0f8777835ab0007',\n",
              " '1e013543291e48369dfbf2ea7ecfde3f',\n",
              " 'a9c24faaae414d6185e1821a20befe3c',\n",
              " '72bfdb5e3f6b4f888c768f838bd2c6e1',\n",
              " 'e937462866d74d32b16dd09d12b452b4',\n",
              " '44c68bbdc2a842a7ab6e28ac35003347',\n",
              " 'b143119a67b64e3b9f0a353c1a690fdf',\n",
              " 'c9a43f8e9e754af6b3b5f12a8118eb3e',\n",
              " 'f45d883469ce47fda4116c2f51b1d0cf',\n",
              " '98c8388b38d34110ac2b04ce3bdc6376',\n",
              " '62cb5ad97d704cecb6fec5399529b400',\n",
              " 'c4813d85f8bb4240ab9a0ca7d17c5ecf',\n",
              " '63d7b5aa620940f7a60da7ce5479cff3',\n",
              " 'ab62e61b137a49a98573ee0250630e69',\n",
              " '0ee516900a334d168bf1c72a359f1769',\n",
              " '344fdd1488394290866ee4363f0adcb8',\n",
              " '8728883243814d928a5230b2b9bf7050',\n",
              " '4cfcb490a3db4477bbcc59b347682db6',\n",
              " '6a618b1eccc044d497d7db0547ca7aa0',\n",
              " '1c177a57a42c4482b9548d89f4549d73',\n",
              " '2ae6c57d323c4d1db8dda591a7c009a6',\n",
              " '137eda42175c42fc97b05fe6ebad6a34',\n",
              " '4f137e0a2afd413ca986ea407d31663a',\n",
              " 'd809532781ce4f9cbb0d6e7c1462b30b',\n",
              " '9568438d564a4503a0905c8666ba7afa',\n",
              " '587f0474cf284f7ab34fcbc84690b8c7',\n",
              " '4ffd4f3e1a8c44d0b87567204f77a999',\n",
              " '263db7e1bda546999356a80b371b4728',\n",
              " '674f49847209444da9e0e8fd1332022e',\n",
              " 'a1a7766b6a4e4bcca589c6c6ef0b0097',\n",
              " '52f9f5f1d3574739aa224e264d70a8ef',\n",
              " '333e488d4d1941e2a2fd1227e50f28e8',\n",
              " '55c1536dec274869a66564303a48191e',\n",
              " '4d35031333b64c8180e9c0532c7d2a15',\n",
              " '06e633dbf0dd4637a2bd72f29ec293d2',\n",
              " 'f9f4e0df2fe348c493c43f0a04202bf0',\n",
              " '94686b40a8fa4e718352d2c619648c1a',\n",
              " '6171721aea334eaea4753680dbfbb51a',\n",
              " 'fde8d8d87a284ada99cd1615134118d1',\n",
              " 'e2b6e891bc354e3fb0671ccfacab336c',\n",
              " '1e3b4bf35e574ca4ac90635a59cb369d',\n",
              " '2420ee56a53f4568972bbb4a1ab6803c',\n",
              " 'f4c29ab91c4c4ec8b9f50e3257a5da2f',\n",
              " '8ad8cd159b544065b9c5e811d7febeba',\n",
              " 'a463f5103a50435f9923e5e9b23dacb0',\n",
              " '1e6eea04290f484b89e11fc75bcbaf39',\n",
              " '5c038870926b41a69f61fcd294e81e39',\n",
              " '68e80a1a678d48f4ac5c3cb5538c6444',\n",
              " '0c69c068d88b4477977a17960acdcf66',\n",
              " '318045fee0f142cf8272f80588d66315',\n",
              " '6805bf3d138f462491c3cad6b1419b35',\n",
              " '1e6d1da87f15428ba1e63716ea618270',\n",
              " '4a45ad4c220c4a02abb999cdf4970e4f',\n",
              " '3628d801616d45f1aa87cc94ed76c3bd',\n",
              " '4ee5b5e7d8ed4431ba0c937fe41d99ec',\n",
              " '49faf0eeb246407290ad9ab3da7330d7',\n",
              " 'd28840f222f1413a82fd4450d49ece54',\n",
              " '632e74cebfeb4cb2b1ec023eb6480ca2',\n",
              " 'c8799536086e446d802330710b8aa03c',\n",
              " 'db9445ff90974ce691e9d91b38bf1a49',\n",
              " '41da3d9e3b2e4d349d01ed1c3856457e',\n",
              " '664564dc43bc44009ce904613c522f4e',\n",
              " '11712cbd3afb4ac880fd5ec6525c63c0',\n",
              " 'bae54788127e45c5b8d43b38181e3202',\n",
              " '3e7248b42d8a4231b06ee763cfe01a22',\n",
              " 'fc01f33db1e7492087b2752d3b911473',\n",
              " '22a1ee587f524f1ba5897704f19a89a4',\n",
              " '0a884c0d03b34124ac0d43f3cfd59ed6',\n",
              " '5b7083a0f06a4f29b3b038b2b633b0ac',\n",
              " '503cb1ded6c6441d894b049d57f9f71b',\n",
              " '9b43e078e1244a0b90e54ffdb8ab880b',\n",
              " 'a981923bc3fb407f8e63cfa50b391fee',\n",
              " '6be531cf7d7a477e8f04fde2cc8cc524',\n",
              " 'ce9b3c2a95904a9f87b72da47ec6dd35',\n",
              " '8661b9332a454d72b78095335033beb4',\n",
              " '80d523e2bd404eeb8219ff97d0c0f7c8',\n",
              " '3e38ed3711e8492b94faf6c07359d65a',\n",
              " '59c84adabcaa476994d2454f25ad3220',\n",
              " '6144a5b117e4468a90a4c80b8131d6e9',\n",
              " 'f390585b643244b489886e443b744eb0',\n",
              " '3c4259f39b6441f99077f5ed7e354abc',\n",
              " '0520d9f0cb2c41ccab9a9246ec624d70',\n",
              " 'f430bb5ba413491bbac1d1b121efedec',\n",
              " 'd91b16288fe34867806749b637897aa8',\n",
              " '62a75560c4d642c383ebd1b42fa6d699',\n",
              " '053e00d13c9b48019b90b8c07a19475b',\n",
              " '5b64bb4907d247c9b0e7b6b0c462cb54',\n",
              " 'ed146639212e428bb023119d9221bfdc',\n",
              " '66e1883f3a0f430eb9821b265aa0b1bb',\n",
              " '97238664d0884553941dfbf3fa205fa2',\n",
              " '160632ca585b4f61a9f1883bc1d5e0a2',\n",
              " '59f9b28774164f6b814c84d33ebbeed4',\n",
              " '1b71988f8d504926ba75b79e7ab0fc57',\n",
              " '46683b3b69694fa684e150f2dfc07adc',\n",
              " '63e694ec8f094d0aabfa31b0df16fa2d',\n",
              " 'a9219c1407ea46b1941361e8c0937a45',\n",
              " 'e5fe508bc4794cac842d46daa65f9cbe',\n",
              " 'a1447e67f1a942f0946b46263db57fb0',\n",
              " 'c69f8b05612d46fa9ea3ba118493e885',\n",
              " '103c14d30f044d6f8f96016bc55c4509',\n",
              " '45645f01f1e54bba96bde99abf96e33e',\n",
              " '3c702add608647608baacf0743616763',\n",
              " 'f6ebf525f71a40cda8f1d7c86805b6b7',\n",
              " '32a8a8336214453b88e45e5c4a2b516a',\n",
              " '15f40ee934b74c098ce62690ecce1477',\n",
              " 'd579cb6721864f08aba6f65afcca5ccb',\n",
              " '0331d81c41bb46ddaff134086d455c73',\n",
              " 'a51f608fd4b44b1e9b60420e3a55971d',\n",
              " '754476b03aac4282bc30585e942d8ff0',\n",
              " '4104772568da4b49b9b681b7008665c8',\n",
              " '914cedf4258445019ab37ce116413e05',\n",
              " 'aec84ad6e3fa4557b3b8f7f79af6d3c4',\n",
              " '035f081b9b1c4afa974f7cdea917730b',\n",
              " '16a26c812f5d4c91b9a0e1ec14bf329e',\n",
              " '16b9090951264165ab8c5d4793b29453',\n",
              " '654c0845af864e58a931a752020ddc32',\n",
              " 'cdefc77ccb1e49b19c54997fdc6cadef',\n",
              " 'c03b179aa86140c1991f9e99da2f572e',\n",
              " 'dc3e84c6f3f94dd58a3ebc4584b2c751',\n",
              " 'ef1a36b95b2e4fa7a97929f954ba1406',\n",
              " 'd2dcf41820494bef924946f70dec6e2d',\n",
              " 'bd280aa728184f08938283296c7fdf3a',\n",
              " '929d562d03ca4116a86810e21c774ca9',\n",
              " '988068e3ef0443338232830fd510d8d8',\n",
              " '1e51105131604eb299c5656c80459246',\n",
              " 'dfd23da7e35544b595b0f166750dde51',\n",
              " '198f1805166e454a93fb15f07e13e1e8',\n",
              " '1b7331e2335142f9896da4e5d24af61d',\n",
              " 'ba53814538124bdda606c91557c363e6',\n",
              " 'd316f4f5f8fa4b32ba6f59ac295a5a2a',\n",
              " 'ba832eb5fb2a4e3f93d58e58109d9a4c',\n",
              " '26695b33f1884ef7932981f5692a4cc7',\n",
              " '130c0c40d4e44c9e809d9187b185e7c0',\n",
              " 'a59353193d9a45bfb2b007f48cae1d78',\n",
              " 'b7a61340a3f74a858591e34f1d15ef1e',\n",
              " 'a72d43cb50d64b7d8f3e05acde797048',\n",
              " '9a60888cdc64401a99a9db15de35f21e',\n",
              " '45ddb382deca4c4b83bb1d86477d927b',\n",
              " 'ff9eb57773ce4696a658aa7b86c6a1ac',\n",
              " 'bb707fcc8876407199763e230094746a',\n",
              " '99ca73bfa5e74c318ce7f5af5b524046',\n",
              " 'a5c9ef36e5ef4e199788587d8277d4d2',\n",
              " '904cfe1149fd4ba18ebaaffb5732e377',\n",
              " 'f68ea74dcff84028b9ef06aea3ee5b54',\n",
              " '353638f1adec49119a61b40dfa85dce8',\n",
              " '514e25416ae0421e88506c1b2f0c6c41',\n",
              " '26dfd10fe4cc4a0eb058c15c63ba421b',\n",
              " '112f3723645b4cf4a1707ccc52f1a024',\n",
              " '24545242f34c40bea7649b8119fdc7f3',\n",
              " 'b95f11eaac9848c3b4d931b5cce5d8cc',\n",
              " '83848c02cd20445a91aae6f4123f4a5e',\n",
              " '8dfd70c30f4b4b568d8d951061bdc768',\n",
              " 'a6780dc06bfe40acb0e94f5516db14a9',\n",
              " '089248ca5f9b4994abbdb9b112e9cf83',\n",
              " '4dcf835a9d1f47c79ac56b35c52b6ab0',\n",
              " '162a454489d243b98b6dc082626d0edc',\n",
              " 'f5210b60a8bc48809579cbb19e5badc8',\n",
              " '2ddea10671ae4645a2a290d3e3d44a3b',\n",
              " 'f2beadc4728d42f5adb59f7238060def',\n",
              " '06e17e61b07d4ed7b3ea4ce930bd4b9b',\n",
              " '524fae469c94487d81d8c034b440e399',\n",
              " '1915f05fc17d497799f4d0e17514eedc',\n",
              " '296bfaf0d43847c88bb997df1a271a25',\n",
              " 'fbe05c7602094a9d95590195e932a788',\n",
              " '0d09782f42c04e33bbaf8fc75181eda8',\n",
              " 'bab36d99b8aa4f8c8b573fe789485af3',\n",
              " 'cf7fe26e5785498e915fbefd51ceaec4',\n",
              " 'e0c43be76215411eb9c9852e2de6316e',\n",
              " 'fe2c1d6c78cf419f9f27984a5a9c8323',\n",
              " '07331935a2284a4e93e14d188161912a',\n",
              " '04aea7842ebe4a83a6e664e218724ae9',\n",
              " '624c9dab870f4ab7a5a4bcd141f165d6',\n",
              " '6c8538f6ac64495d922d58231562f8fd',\n",
              " 'f61a7655125d4ee9978754835fc85d24',\n",
              " '07aff60ac9264abc99e42e132e0c083c',\n",
              " '059f6ca01b544014bcd2942e9b7ea566',\n",
              " '38b8665100344cb897dc1d65efdcbf13',\n",
              " 'e19bb6b3a45747aaa6bb6e18e1eec39f',\n",
              " '7ae68eb12bb64a6398cd8f542749ccef',\n",
              " 'b43258a52ce94c23a2d49ff95b0abef1',\n",
              " 'f6d48e70c9bc42428e3e0e8d1c03c1bd',\n",
              " '88b1bbb5eb4e4470948cad18671ef58a',\n",
              " '7e281a819cbe49619388965f1134e35c',\n",
              " '025db7d76e6842bf89aff7a230cecc39',\n",
              " '7448eba1558b49798da64db325139430',\n",
              " '8af6b44813f040798bceac19cf46f3cf',\n",
              " '8c47fdeb1345407281b7a17c2b859391',\n",
              " '2f9aea64171a400288dc3ddc2235cd8e',\n",
              " '6b1c1e22bb7046da8067a6bb5046aa16',\n",
              " '74d43686986244f88d9e6974ad0c67f3',\n",
              " 'a0c564ae9da244c197c3256fdee61381',\n",
              " 'a3c61abd1a31485d8ddaaeacb9512a92',\n",
              " 'dda7f256c58f406681746a1fa4ecbc57',\n",
              " '6157fb37209b4fd4ac22a1d543a49d0e',\n",
              " 'b926829702c241a3aeddf500dd110b04',\n",
              " 'df43fb385134461c857d776c19188001',\n",
              " '25cad43a1a7c4e299b024b4dc8da28b8',\n",
              " 'b6a072896cbd42f3b83ac29be5d0e701',\n",
              " 'bac78ab1b6a24f16bf64ac66e6249dac',\n",
              " '68f3926c3e874bcb83e12ef9da99c4c0',\n",
              " '8de6cddd68d040f38e03bbd0e92a912e',\n",
              " '3e6ed4c54bb34eb290114ae8f54e4a9c',\n",
              " '1f5e5191c30d4d76b9aa44131d8e2dbe',\n",
              " '0454497bb3434f17a08e9593f23ac078',\n",
              " '7ca17953e1014a30a136bfa9dc24ba8d',\n",
              " '3134047b9ec34520b105acacb85fd1df',\n",
              " '858d8b2ab37c45eabcdf08fd2ccb2d0c',\n",
              " 'b9f0e6216fcf4b699ebf6d364deaf67c',\n",
              " '327fb40fdc1240fe90dd6d9ae8e44632',\n",
              " 'e507d74aaa2f4d0ca0ee5e9670ebd9c1',\n",
              " 'b10b68c864ca45c49420816ebeb14992',\n",
              " '484f205151e947999366307fe1be25d1',\n",
              " '11856c7f71034f06b0d3ff32db0105dd',\n",
              " '2df58852823c4da0b9d6a2d2fb2de466',\n",
              " '4d15e70ec3e9413ca4cb9881e9c4f03b',\n",
              " '762dccac1d0d4023acc646122bfc147c',\n",
              " '1c612be5e10c492da658d5b7667f703e',\n",
              " '5a4a65bc698b4a8587a114f8084e8440',\n",
              " '0a10f6ee3c8247b681f7b4dcfc2ba503',\n",
              " '07788cb322aa42a7a9e8ef31fff3da40',\n",
              " 'f8e075f3831e491a8fe773608acdda1a',\n",
              " 'adf93981022243e99a6c6b6858bb0d4e',\n",
              " '59508939ceca4e0191ebbdee23421437',\n",
              " '8e0422a1d6c94333825e41c6f0011c98',\n",
              " 'a45f398d144847b89dca76b1c57e19ae',\n",
              " 'ee3b6d7de38b4903a4a7daf010c0a349',\n",
              " 'a1c847e6d0a3470d8a303377e30e687e',\n",
              " '5f0c88c6f77646378a86fda0fb6d8972',\n",
              " '2484abcd16f3487fa3ae13a9b8547a1f',\n",
              " '38fd20402c454d2db0fa4286d9c5768d',\n",
              " '127e097bf4f54eeba9bfa5eda2fe8d70',\n",
              " '84a027ba5a57442e9e7087ba0be64a75',\n",
              " '2f5e742ec5334cdcbec4d1c08fca8ddb',\n",
              " '23ffcd90f21b4b4aa4d507a96c34ba6e',\n",
              " '0e771ab3d8494990af24a6cd68ae9325',\n",
              " 'ae7110fe29384ac7b877f025520315b5',\n",
              " 'c46f8ca5622b445a9598a01ed94f8a00',\n",
              " 'd39547692f1240ccbc27f343bb02fe3e',\n",
              " 'f4528ad27b164a25b9480145d1541abe',\n",
              " '05513ddb46c944178715a9d3a9dd702a',\n",
              " 'ce5041d85ac740fdb2a1d6e42494ed58',\n",
              " 'bd58f20177e34f23b9a2273b1c892cd3',\n",
              " '9f854afdb4d447bb9bece7aa8f176e1d',\n",
              " 'df1ac646c20b4c61bc998cc012a46edf',\n",
              " '6a1431a8a67646848ab99faef7e47304',\n",
              " 'e3edc332c9e847e083bcb37b4dbccc3d',\n",
              " '69519a06e30d4532a9cd74114443d1f4']"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import VectorParams, Distance\n",
        "\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"fusion-rag27-dec\",\n",
        "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
        ")\n",
        "\n",
        "vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=\"fusion-rag27-dec\",\n",
        "    embedding=embeddings,\n",
        ")\n",
        "vector_store.add_documents(doc_splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "hNrRAVElCXVX"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmBcPANiGBjm",
        "outputId": "c6de3383-ed6b-4f40-d400-832b1adf15db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'title': 'Understanding VGG Neural Networks: Architecture and Implementation | by Anna Alexandra Grigoryan | Medium', 'description': 'VGG (Visual Geometry Group) is a convolutional neural network architecture that was proposed by researchers from the University of Oxford in 2014. It gained popularity and recognition for its…', 'keywords': None, 'author': 'Anna Alexandra Grigoryan', 'og:site_name': 'Medium', 'og:type': 'article', 'og:title': 'Understanding VGG Neural Networks: Architecture and Implementation', 'og:description': 'VGG’s depth-powered brilliance.', 'og:url': 'https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba', 'og:image': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:app:name:iphone': 'Medium', 'twitter:app:id:iphone': '828256236', 'twitter:site': '@Medium', 'twitter:app:url:iphone': 'medium://p/400d99a9e9ba', 'twitter:image:src': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:card': 'summary_large_image', 'twitter:label1': 'Reading time', 'twitter:data1': '6 min read', '_id': '6144a5b117e4468a90a4c80b8131d6e9', '_collection_name': 'fusion-rag27-dec'}, page_content='VGG architectures outperform the existing state-of-the-art models on various benchmark datasets, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) dataset, demonstrating the effectiveness of deeper architectures for image classification.\\\\n\\\\n# VGG Blocks: Building Depth\\\\n\\\\nA VGG block is a modular unit within the VGG architecture that encapsulates a set of convolutional and pooling layers. These layers, operating in tandem, capture features of varying complexity, from basic edges and shapes to more elaborate textures and patterns. VGG recognized that by stacking these blocks together, the network could learn to discern high-level features that define the identity of objects in images.\\\\n\\\\nThe pivotal role of VGG blocks lies in their ability to add depth to the neural network. Depth, in this context, refers to the number of layers within the network. Deeper networks can capture more intricate and abstract features, enabling them to distinguish subtle differences in images.\\\\n\\\\nBy stacking multiple VGG blocks, the network evolves from a relatively shallow structure to a deep, hierarchical architecture.\\\\n\\\\nVGG takes a systematic approach to configuring the blocks, ensuring a consistent and scalable'),\n",
              " Document(metadata={'title': 'Understanding VGG Neural Networks: Architecture and Implementation | by Anna Alexandra Grigoryan | Medium', 'description': 'VGG (Visual Geometry Group) is a convolutional neural network architecture that was proposed by researchers from the University of Oxford in 2014. It gained popularity and recognition for its…', 'keywords': None, 'author': 'Anna Alexandra Grigoryan', 'og:site_name': 'Medium', 'og:type': 'article', 'og:title': 'Understanding VGG Neural Networks: Architecture and Implementation', 'og:description': 'VGG’s depth-powered brilliance.', 'og:url': 'https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba', 'og:image': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:app:name:iphone': 'Medium', 'twitter:app:id:iphone': '828256236', 'twitter:site': '@Medium', 'twitter:app:url:iphone': 'medium://p/400d99a9e9ba', 'twitter:image:src': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:card': 'summary_large_image', 'twitter:label1': 'Reading time', 'twitter:data1': '6 min read', '_id': 'f5210b60a8bc48809579cbb19e5badc8', '_collection_name': 'fusion-rag27-dec'}, page_content='The VGG19 model, with 19 layers, performs well on various image classification tasks, but even the simpler architectures like VGG16 and VGG13 provide competitive results.\\\\n\\\\nThe networks are trained using stochastic gradient descent (SGD) with momentum. Dropout, a regularization technique, is also applied to reduce overfitting.\\\\n\\\\nThe VGG architectures outperform the existing state-of-the-art models on various benchmark datasets, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) dataset, demonstrating the effectiveness of deeper architectures for image classification.\\\\n\\\\n# VGG Blocks: Building Depth\\\\n\\\\nA VGG block is a modular unit within the VGG architecture that encapsulates a set of convolutional and pooling layers. These layers, operating in tandem, capture features of varying complexity, from basic edges and shapes to more elaborate textures and patterns. VGG recognized that by stacking these blocks together, the network could learn to discern high-level features that define the identity of objects in images.\\\\n\\\\nThe pivotal role of VGG blocks lies in their ability to add depth to the neural network. Depth, in this context,'),\n",
              " Document(metadata={'title': 'Understanding VGG Neural Networks: Architecture and Implementation | by Anna Alexandra Grigoryan | Medium', 'description': 'VGG (Visual Geometry Group) is a convolutional neural network architecture that was proposed by researchers from the University of Oxford in 2014. It gained popularity and recognition for its…', 'keywords': None, 'author': 'Anna Alexandra Grigoryan', 'og:site_name': 'Medium', 'og:type': 'article', 'og:title': 'Understanding VGG Neural Networks: Architecture and Implementation', 'og:description': 'VGG’s depth-powered brilliance.', 'og:url': 'https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba', 'og:image': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:app:name:iphone': 'Medium', 'twitter:app:id:iphone': '828256236', 'twitter:site': '@Medium', 'twitter:app:url:iphone': 'medium://p/400d99a9e9ba', 'twitter:image:src': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:card': 'summary_large_image', 'twitter:label1': 'Reading time', 'twitter:data1': '6 min read', '_id': '25cad43a1a7c4e299b024b4dc8da28b8', '_collection_name': 'fusion-rag27-dec'}, page_content='epochs=<span>10</span>)</span></pre><p>The model is compiled with the optimizer, loss function, and metrics. In this case, the Adam optimiser, sparse categorical cross-entropy loss, and accuracy metric are used.</p><p>Make sure to preprocess the dataset of images by resizing and normalising and then creating batches for training.</p><h1>Final Notes</h1><p>VGG’s deep architecture enabled it to surpass earlier models by accurately classifying diverse objects within images. The systematic layering of convolutional blocks paved the way for capturing intricate features, contributing to its superior accuracy.</p><p>However, VGGs are resource-intensive computations and the demand for computational resources limit its scalability, making it less practical for deployment on resource-constrained devices.</p><p>To overcome these limitations, the concept of transfer learning gained relevance. Developers can fine-tune VGG models pre-trained on large datasets on domain-specific data, achieving impressive results with significantly less training'),\n",
              " Document(metadata={'title': 'Understanding VGG Neural Networks: Architecture and Implementation | by Anna Alexandra Grigoryan | Medium', 'description': 'VGG (Visual Geometry Group) is a convolutional neural network architecture that was proposed by researchers from the University of Oxford in 2014. It gained popularity and recognition for its…', 'keywords': None, 'author': 'Anna Alexandra Grigoryan', 'og:site_name': 'Medium', 'og:type': 'article', 'og:title': 'Understanding VGG Neural Networks: Architecture and Implementation', 'og:description': 'VGG’s depth-powered brilliance.', 'og:url': 'https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba', 'og:image': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:app:name:iphone': 'Medium', 'twitter:app:id:iphone': '828256236', 'twitter:site': '@Medium', 'twitter:app:url:iphone': 'medium://p/400d99a9e9ba', 'twitter:image:src': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:card': 'summary_large_image', 'twitter:label1': 'Reading time', 'twitter:data1': '6 min read', '_id': '88871048c35349edb85de19fee5f13d2', '_collection_name': 'fusion-rag27-dec'}, page_content='acknowledge the increased computational cost. The VGG19 model, with 19 layers, performs well on various image classification tasks, but even the simpler architectures like VGG16 and VGG13 provide competitive results.\\\\n\\\\nThe networks are trained using stochastic gradient descent (SGD) with momentum. Dropout, a regularization technique, is also applied to reduce overfitting.\\\\n\\\\nThe VGG architectures outperform the existing state-of-the-art models on various benchmark datasets, including the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) dataset, demonstrating the effectiveness of deeper architectures for image classification.\\\\n\\\\n# VGG Blocks: Building Depth\\\\n\\\\nA VGG block is a modular unit within the VGG architecture that encapsulates a set of convolutional and pooling layers. These layers, operating in tandem, capture features of varying complexity, from basic edges and shapes to more elaborate textures and patterns. VGG recognized that by stacking these blocks together, the network could learn to discern high-level features that define the identity of objects in images.\\\\n\\\\nThe pivotal role of VGG blocks lies in their ability to add depth to the neural network.')]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "retriever.invoke('what is vgg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "to9tvXqXM0QV"
      },
      "outputs": [],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "model = ChatMistralAI(model=\"open-mistral-nemo\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke('what is ai')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTVVsiv-g-nn",
        "outputId": "1f9d2585-ce14-4d48-bca9-f78e0f9fdbfc"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Artificial Intelligence (AI) is a broad field of computer science dedicated to creating machines that can carry out tasks in a way that we would consider \"smart\" or \"intelligent\". Here are some key aspects of AI:\\n\\n1. **Machine Learning (ML)**: This is a subset of AI that involves training models on data to make predictions or decisions without being explicitly programmed. It\\'s how systems like Netflix recommend movies or Amazon suggests products.\\n\\n2. **Deep Learning (DL)**: This is a subset of ML that uses artificial neural networks with many layers to extract high-level features from raw input. For example, image classification or speech recognition.\\n\\n3. **Natural Language Processing (NLP)**: This is a subfield of AI that focuses on the interaction between computers and human language. It enables computers to understand, interpret, and generate human language.\\n\\n4. **Computer Vision**: This is an AI field that focuses on enabling computers to interpret and understand the visual world, through images and videos.\\n\\n5. **Robotics**: This involves designing and building robots, often using AI to control them and enable them to interact with their environment.\\n\\n6. **Expert Systems**: These are AI programs that can make decisions or solve problems based on the knowledge and expertise of human experts.\\n\\nAI is used in a wide range of applications, from voice assistants like Siri and Alexa, to recommendation systems, autonomous vehicles, and more. However, it\\'s important to note that while AI can perform tasks that typically require human intelligence, it doesn\\'t possess consciousness, self-awareness, or a \"mind\" in the human sense.\\n\\nHere are a few simple examples of AI in action:\\n\\n- When you search for something on Google, the results are ranked by an AI algorithm.\\n- When you ask Siri or Alexa a question, an AI system processes your speech and generates a response.\\n- When you play a game like chess against a computer, you\\'re playing against an AI program.', additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 6, 'total_tokens': 401, 'completion_tokens': 395}, 'model': 'open-mistral-nemo', 'finish_reason': 'stop'}, id='run-dc50c143-722f-4732-b9fa-2a4b86839559-0', usage_metadata={'input_tokens': 6, 'output_tokens': 395, 'total_tokens': 401})"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "query_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=(\n",
        "        \"\"\"You are an expert in modifying queries for better retrieval,\\n\\n\n",
        "        Here is one query modify 3 similar queries for better information retrieval and generation\\n\\n\n",
        "        Query:{query}\\n\\n\n",
        "        Generated query:\\n\\n\"\"\"\n",
        "\n",
        "    ))"
      ],
      "metadata": {
        "id": "50si9xn4hR4n"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_chain = query_prompt | model"
      ],
      "metadata": {
        "id": "5iwaO1-Fh3LX"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_query = query_chain.invoke('what is vgg16 according to the documents?')"
      ],
      "metadata": {
        "id": "biuoixiciBG2"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "queries = re.findall(r'\"(.*?)\"', original_query.content)\n",
        "\n",
        "queries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKUaqkVEiQHm",
        "outputId": "c5b82066-d71d-4dd5-eac9-e908c921ab0e"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Provide a brief definition of VGG16 from the documents.',\n",
              " 'Describe the architectural details of VGG16 as mentioned in the documents.',\n",
              " 'What are the primary use cases or applications of VGG16, as discussed in the documents?']"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever\n",
        "\n",
        "retriever1 = BM25Retriever.from_documents(doc_splits)\n",
        "retriever1.invoke('what is vgg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiTEW9_nkYdX",
        "outputId": "f3bcda75-1571-4734-af62-da7ffabdf311"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'title': 'Understanding VGG Neural Networks: Architecture and Implementation | by Anna Alexandra Grigoryan | Medium', 'description': 'VGG (Visual Geometry Group) is a convolutional neural network architecture that was proposed by researchers from the University of Oxford in 2014. It gained popularity and recognition for its…', 'keywords': None, 'author': 'Anna Alexandra Grigoryan', 'og:site_name': 'Medium', 'og:type': 'article', 'og:title': 'Understanding VGG Neural Networks: Architecture and Implementation', 'og:description': 'VGG’s depth-powered brilliance.', 'og:url': 'https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba', 'og:image': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:app:name:iphone': 'Medium', 'twitter:app:id:iphone': '828256236', 'twitter:site': '@Medium', 'twitter:app:url:iphone': 'medium://p/400d99a9e9ba', 'twitter:image:src': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:card': 'summary_large_image', 'twitter:label1': 'Reading time', 'twitter:data1': '6 min read'}, page_content=\"classification head is defined with a `Flatten` layer. Then, a fully connected (`Dense`) layer with 256 units and ReLU activation is added, followed by the final classifier layer with the number of units equal to the `num_classes` and a softmax activation function.\\\\n\\\\nIn the `call` method, the input tensor is passed through each block sequentially, followed by the flatten, fully connected, and classifier layers, creating a sequential chain of layers that define the forward pass of the network.\\\\n\\\\n# Training\\\\n\\\\nHere’s a brief explanation of the steps involved in the process of training the custom VGG model.\\\\n\\\\n```\\\\n# Initialize VGG with the number of classes vgg = MyVGG(num_classes=2)\\\\u200b# Compile with losses and metricsvgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])# Train the custom VGG modelvgg.fit(dataset, epochs=10)\\\\n```\\\\n\\\\n\\\\nThe model\"),\n",
              " Document(metadata={'title': 'Understanding VGG Neural Networks: Architecture and Implementation | by Anna Alexandra Grigoryan | Medium', 'description': 'VGG (Visual Geometry Group) is a convolutional neural network architecture that was proposed by researchers from the University of Oxford in 2014. It gained popularity and recognition for its…', 'keywords': None, 'author': 'Anna Alexandra Grigoryan', 'og:site_name': 'Medium', 'og:type': 'article', 'og:title': 'Understanding VGG Neural Networks: Architecture and Implementation', 'og:description': 'VGG’s depth-powered brilliance.', 'og:url': 'https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba', 'og:image': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:app:name:iphone': 'Medium', 'twitter:app:id:iphone': '828256236', 'twitter:site': '@Medium', 'twitter:app:url:iphone': 'medium://p/400d99a9e9ba', 'twitter:image:src': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:card': 'summary_large_image', 'twitter:label1': 'Reading time', 'twitter:data1': '6 min read'}, page_content=\"and classifier layers, creating a sequential chain of layers that define the forward pass of the network.\\\\n\\\\n# Training\\\\n\\\\nHere’s a brief explanation of the steps involved in the process of training the custom VGG model.\\\\n\\\\n```\\\\n# Initialize VGG with the number of classes vgg = MyVGG(num_classes=2)\\\\u200b# Compile with losses and metricsvgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])# Train the custom VGG modelvgg.fit(dataset, epochs=10)\\\\n```\\\\n\\\\n\\\\nThe model is compiled with the optimizer, loss function, and metrics. In this case, the Adam optimiser, sparse categorical cross-entropy loss, and accuracy metric are used.\\\\n\\\\nMake sure to preprocess the dataset of images by resizing and normalising and then creating batches for training.\\\\n\\\\n# Final Notes\\\\n\\\\nVGG’s deep architecture enabled it to surpass earlier models by accurately classifying diverse objects\"),\n",
              " Document(metadata={'title': 'Understanding VGG Neural Networks: Architecture and Implementation | by Anna Alexandra Grigoryan | Medium', 'description': 'VGG (Visual Geometry Group) is a convolutional neural network architecture that was proposed by researchers from the University of Oxford in 2014. It gained popularity and recognition for its…', 'keywords': None, 'author': 'Anna Alexandra Grigoryan', 'og:site_name': 'Medium', 'og:type': 'article', 'og:title': 'Understanding VGG Neural Networks: Architecture and Implementation', 'og:description': 'VGG’s depth-powered brilliance.', 'og:url': 'https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba', 'og:image': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:app:name:iphone': 'Medium', 'twitter:app:id:iphone': '828256236', 'twitter:site': '@Medium', 'twitter:app:url:iphone': 'medium://p/400d99a9e9ba', 'twitter:image:src': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:card': 'summary_large_image', 'twitter:label1': 'Reading time', 'twitter:data1': '6 min read'}, page_content=\"Training\\\\n\\\\nHere’s a brief explanation of the steps involved in the process of training the custom VGG model.\\\\n\\\\n```\\\\n# Initialize VGG with the number of classes vgg = MyVGG(num_classes=2)\\\\u200b# Compile with losses and metricsvgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])# Train the custom VGG modelvgg.fit(dataset, epochs=10)\\\\n```\\\\n\\\\n\\\\nThe model is compiled with the optimizer, loss function, and metrics. In this case, the Adam optimiser, sparse categorical cross-entropy loss, and accuracy metric are used.\\\\n\\\\nMake sure to preprocess the dataset of images by resizing and normalising and then creating batches for training.\\\\n\\\\n# Final Notes\\\\n\\\\nVGG’s deep architecture enabled it to surpass earlier models by accurately classifying diverse objects within images. The systematic layering of convolutional blocks paved the way for capturing intricate features, contributing to its superior\"),\n",
              " Document(metadata={'title': 'Understanding VGG Neural Networks: Architecture and Implementation | by Anna Alexandra Grigoryan | Medium', 'description': 'VGG (Visual Geometry Group) is a convolutional neural network architecture that was proposed by researchers from the University of Oxford in 2014. It gained popularity and recognition for its…', 'keywords': None, 'author': 'Anna Alexandra Grigoryan', 'og:site_name': 'Medium', 'og:type': 'article', 'og:title': 'Understanding VGG Neural Networks: Architecture and Implementation', 'og:description': 'VGG’s depth-powered brilliance.', 'og:url': 'https://thegrigorian.medium.com/understanding-vgg-neural-networks-architecture-and-implementation-400d99a9e9ba', 'og:image': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:app:name:iphone': 'Medium', 'twitter:app:id:iphone': '828256236', 'twitter:site': '@Medium', 'twitter:app:url:iphone': 'medium://p/400d99a9e9ba', 'twitter:image:src': 'https://miro.medium.com/v2/resize:fit:1024/1*1MDndESIYun5h9LjJZ7k9w.png', 'twitter:card': 'summary_large_image', 'twitter:label1': 'Reading time', 'twitter:data1': '6 min read'}, page_content='used OpenAI’s o1 model to develop a trading strategy. It is DESTROYING the market⟨64⟩]\\\\n\\\\n![DataDrivenInvestor⟨65⟩](https://medium.datadriveninvestor.com/?source=post_page---read_next_recirc--400d99a9e9ba----3---------------------738f1f4d_21b7_4c99_a3af_434d7ee3ffa4-------)\\\\n\\\\nIn\\\\n\\\\nDataDrivenInvestor⟨66⟩\\\\n\\\\nby\\\\n\\\\nAustin Starks⟨67⟩\\\\n\\\\n## I used OpenAI’s o1 model to develop a trading strategy. It is DESTROYING the marketIt literally took one try. I was shocked.⟨68⟩\\\\n\\\\nSep')]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[retriever1, retriever], weights=[0.5, 0.5])\n",
        "retrievers = [retriever,retriever1,ensemble_retriever]"
      ],
      "metadata": {
        "id": "PuSsPEXqk2FV"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'what is vgg'\n",
        "def retrieve_docs(retriever,query):\n",
        "  return retriever.invoke(query)\n",
        "r_docs = []\n",
        "for query in queries:\n",
        "  q_docs = []\n",
        "  for retriever in retrievers:\n",
        "    docs = retrieve_docs(retriever,query)\n",
        "    q_docs.append(docs)\n",
        "  q_docs = [x for i in q_docs for x in i]\n",
        "  r_docs.append(q_docs)\n",
        "r_docs = [x for i in r_docs for x in i]"
      ],
      "metadata": {
        "id": "rhtsZ9Lapnx0"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def reranked_docs(docs):\n",
        "  docs_content = [doc.page_content for doc in docs]\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  doc_vectors = vectorizer.fit_transform(docs_content)\n",
        "  query_vector = vectorizer.transform([query])\n",
        "  similarity = cosine_similarity(query_vector,doc_vectors).flatten()\n",
        "  scored_docs = sorted(zip(docs_content, similarity), key=lambda x: x[1], reverse=True)\n",
        "  return scored_docs\n",
        "ranked_docs = reranked_docs(r_docs)[:3]"
      ],
      "metadata": {
        "id": "rRPW0yNktVEz"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatMistralAI(model=\"open-mistral-nemo\")\n",
        "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "rag_chain = rag_prompt | llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJDJoMdFu0CS",
        "outputId": "f3c580c6-49fb-454e-d0ab-9bfe1e585125"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke({'context':ranked_docs,'question':query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjnJCzP0wqmy",
        "outputId": "f845c8ce-a4d2-4639-d813-0372437863d4"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"VGG16 is primarily used for image classification tasks due to its depth and ability to capture intricate features. It's also used in object detection and recognition, especially in cluttered scenes.\", additional_kwargs={}, response_metadata={'token_usage': {'prompt_tokens': 893, 'total_tokens': 931, 'completion_tokens': 38}, 'model': 'open-mistral-nemo', 'finish_reason': 'stop'}, id='run-7291ec59-b76d-43c2-bbdb-fafb30fa501a-0', usage_metadata={'input_tokens': 893, 'output_tokens': 38, 'total_tokens': 931})"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MMeDGDuPw0Wh"
      },
      "execution_count": 150,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}