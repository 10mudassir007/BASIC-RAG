{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "SpE_ooy-uUPQ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain_community tiktoken langchain-groq rank_bm25 langchainhub chromadb langchain langgraph tavily-python langchain-huggingface -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Hhr_rxuWuXdJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.schema.runnable import Runnable\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "from typing import cast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "yvvIp0p-uYiH"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "E17BstKuuau3"
      },
      "outputs": [],
      "source": [
        "# groq_key = os.getenv(\"GROQ_API_KEY\")\n",
        "# tavily_key = os.getenv(\"TAVILY_API_KEY\")\n",
        "from google.colab import userdata\n",
        "groq_key = userdata.get('GROQ_API_KEY')\n",
        "tavily_key =userdata.get('TAVILY_API_KEY'),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "_fj7AdJOWEu0"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://medium.com/@muddassir10/machine-learning-based-price-estimation-a-practical-approach-7164b35d10fd\",\n",
        "]\n",
        "loader = WebBaseLoader(urls)\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "_3r03316we7Q"
      },
      "outputs": [],
      "source": [
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "# Add to vectorDB\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "-vOgvewf8GC6"
      },
      "outputs": [],
      "source": [
        "def reranked_docs(query,docs,k=3):\n",
        "  if any(isinstance(doc, str) for doc in docs):\n",
        "    tokenized_corpus = [doc.split() for doc in docs]\n",
        "  else:\n",
        "    tokenized_corpus = [doc.page_content.split() for doc in docs]\n",
        "  bm25 = BM25Okapi(tokenized_corpus)\n",
        "  scores = bm25.get_scores(query.split())\n",
        "  if sum([1 for score in scores if score > .4]) < 3:\n",
        "    return \"no relevant docs found\"\n",
        "  else:\n",
        "    scored_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
        "  return scored_docs[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "nLAIInRTZIlT"
      },
      "outputs": [],
      "source": [
        "rag_llm = llm = ChatGroq(\n",
        "        api_key=groq_key,\n",
        "        model='llama-3.2-1b-preview',\n",
        "        temperature=0,\n",
        "        max_retries=3,\n",
        "        timeout=None,\n",
        "        max_tokens=512\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "yijtXWmXwjs4"
      },
      "outputs": [],
      "source": [
        "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "rag_chain = rag_prompt | rag_llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "z3QSVJZS3TRD"
      },
      "outputs": [],
      "source": [
        "def generate(query,doc,prompt):\n",
        "  rag_chain = prompt | rag_llm | StrOutputParser()\n",
        "  generation = rag_chain.invoke({\"context\": doc, \"question\": query})\n",
        "  return generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "U-IFda5cwUzJ"
      },
      "outputs": [],
      "source": [
        "prompt1 = '''You are an assistant for question-answering tasks.\n",
        "                Use the following documents to answer the question.\n",
        "                If you don't know the answer, just say that you don't know.\n",
        "                Use three sentences maximum and keep the answer concise:\n",
        "                Question: {question}\n",
        "                Context: {context}\n",
        "                Answer:\n",
        "'''\n",
        "\n",
        "prompt2 = \"\"\"\n",
        "You are a helpful assistant.\n",
        "            Answer the question according to the query and given context:\n",
        "            Question: {question}\n",
        "            Context: {context}\n",
        "            Provide an accurate response in bullet points but don't mention it in the response,\n",
        "            the answer should be brief (max 5 lines/points).\n",
        "            Do not hallucinate.\n",
        "\"\"\"\n",
        "\n",
        "prompt3 =\"\"\"\n",
        "\"You are an assistant for question-answering tasks. Use the following context extracted from a webpage to answer the question.\n",
        "If you don't know the answer, just say that you don't know. Keep the answer concise and brief.\"\n",
        "Question: {question}\n",
        "Context:{context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt4 =\"\"\"\n",
        "Given the context provided, respond to the question accurately, using only the information in the context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Response:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompts = [rag_prompt,prompt1,prompt2,prompt3,prompt4]\n",
        "\n",
        "for prompt_id in range(len(prompts)):\n",
        "  try:\n",
        "    prompts[prompt_id] = ChatPromptTemplate.from_template(prompts[prompt_id])\n",
        "  except:\n",
        "    continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ4L58kvxpvY",
        "outputId": "7c759a06-320b-4737-a538-67a45f0c520a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main processing steps in a machine learning project include:\n",
            "\n",
            "1. Data collection and preprocessing\n",
            "2. Feature engineering\n",
            "3. Model selection and training\n",
            "4. Model evaluation and tuning\n",
            "5. Model deployment and integration\n",
            "\n",
            "These steps are applied to various machine learning algorithms, such as linear regression, gradient boosting regressor, decision tree regressor, AdaBoost regressor, and stochastic gradient descent, to develop a comprehensive multiple domain pricing estimator.\n",
            "The main processing steps in a machine learning project are:\n",
            "\n",
            "1. Data collection and preprocessing\n",
            "2. Feature engineering\n",
            "3. Model selection and training\n",
            "4. Model evaluation and tuning\n",
            "5. Model deployment and integration\n",
            "\n",
            "These steps are crucial in building a comprehensive multiple domain pricing estimator.\n",
            "• Collect and preprocess data from various sources.\n",
            "• Employ Exploratory Data Analysis (EDA) to identify patterns and outliers.\n",
            "• Feature engineer and scale the data for algorithmic modeling.\n",
            "• Apply domain-specific algorithms for each domain.\n",
            "• Fine-tune hyperparameters using GridSearchCV.\n",
            "• Evaluate model performance using scores and API craftmanship.\n",
            "The main processing steps in a machine learning project are:\n",
            "\n",
            "1. Data collection and preprocessing\n",
            "2. Feature engineering\n",
            "3. Model selection and training\n",
            "4. Model evaluation and tuning\n",
            "5. Model deployment and integration\n",
            "\n",
            "These steps are applied to various domains, including houses, cars, laptops, and mobiles.\n",
            "The main processing steps in a machine learning (ML) project typically involve the following:\n",
            "\n",
            "1. **Data Collection**: Gathering relevant data from various sources, including datasets, APIs, or user input.\n",
            "2. **Data Preprocessing**: Cleaning, transforming, and preparing the data for modeling by handling missing values, encoding categorical variables, and scaling/normalizing the data.\n",
            "3. **Feature Engineering**: Creating new features or transforming existing ones to improve model performance, such as extracting relevant information from text data or creating new variables.\n",
            "4. **Model Selection**: Choosing the most suitable machine learning algorithm or model type for the problem, considering factors like data type, complexity, and performance requirements.\n",
            "5. **Model Training**: Training the selected model using the preprocessed data, often involving iterative optimization and hyperparameter tuning.\n",
            "6. **Model Evaluation**: Assessing the performance of the trained model using metrics like accuracy, precision, recall, F1-score, and others, to determine its effectiveness.\n",
            "7. **Hyperparameter Tuning**: Adjusting the model's hyperparameters to optimize its performance, often using techniques like GridSearchCV or RandomizedSearchCV.\n",
            "8. **Model Deployment**: Integrating the trained model into a production-ready system, such as a web application or API, to make predictions or take actions.\n",
            "9. **Model Monitoring**: Continuously monitoring the model's performance over time to identify areas for improvement and make data-driven decisions.\n",
            "\n",
            "These steps are not exhaustive, and the specific processing steps may vary depending on the project's requirements, data characteristics, and the chosen ML algorithm.\n"
          ]
        }
      ],
      "source": [
        "query = \"what are the main processessing steps in a ml project?\"\n",
        "reranked_doc = reranked_docs(query,doc_splits)\n",
        "responses = []\n",
        "for prompt in prompts:\n",
        "  response = generate(query,reranked_doc,prompt)\n",
        "  responses.append(response)\n",
        "  print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Eahr2JH4xq6m"
      },
      "outputs": [],
      "source": [
        "def get_best_response(responses):\n",
        "  tokenized_corpus = [response.split() for response in responses]\n",
        "  bm25 = BM25Okapi(tokenized_corpus)\n",
        "  scores = bm25.get_scores(query.split())\n",
        "  scored_responses= sorted(zip(responses, scores), key=lambda x: x[1], reverse=True)\n",
        "  return scored_responses[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmlcwpSJ6eJ0",
        "outputId": "3dd61c5b-0f84-4b57-c20f-1122a262b774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main processing steps in a machine learning (ML) project typically involve the following:\n",
            "\n",
            "1. **Data Collection**: Gathering relevant data from various sources, including datasets, APIs, or user input.\n",
            "2. **Data Preprocessing**: Cleaning, transforming, and preparing the data for modeling by handling missing values, encoding categorical variables, and scaling/normalizing the data.\n",
            "3. **Feature Engineering**: Creating new features or transforming existing ones to improve model performance, such as extracting relevant information from text data or creating new variables.\n",
            "4. **Model Selection**: Choosing the most suitable machine learning algorithm or model type for the problem, considering factors like data type, complexity, and performance requirements.\n",
            "5. **Model Training**: Training the selected model using the preprocessed data, often involving iterative optimization and hyperparameter tuning.\n",
            "6. **Model Evaluation**: Assessing the performance of the trained model using metrics like accuracy, precision, recall, F1-score, and others, to determine its effectiveness.\n",
            "7. **Hyperparameter Tuning**: Adjusting the model's hyperparameters to optimize its performance, often using techniques like GridSearchCV or RandomizedSearchCV.\n",
            "8. **Model Deployment**: Integrating the trained model into a production-ready system, such as a web application or API, to make predictions or take actions.\n",
            "9. **Model Monitoring**: Continuously monitoring the model's performance over time to identify areas for improvement and make data-driven decisions.\n",
            "\n",
            "These steps are not exhaustive, and the specific processing steps may vary depending on the project's requirements, data characteristics, and the chosen ML algorithm.\n"
          ]
        }
      ],
      "source": [
        "print(get_best_response(responses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "IpDtNKklDx1t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}